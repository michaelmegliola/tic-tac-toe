{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Learning\n",
    "\n",
    "Here is a maze, with the entrance marked __e__ (coordinates 0,0) and the exit marked __x__ (coordinates 3,3); the spaces containing 1's are blocked, while the spaces containing 0's are open:\n",
    "<pre>\n",
    "[e 0 0 1]\n",
    "[0 1 0 0]\n",
    "[0 0 1 0]\n",
    "[1 1 0 x]\n",
    "</pre>\n",
    "\n",
    "The goal is to traverse the maze by moving North, South, East,or West using a __Q Learning__ alogrithm.\n",
    "\n",
    "Note that there is one path through the maze:\n",
    "<pre>\n",
    "[e . . 1]\n",
    "[0 1 . .]\n",
    "[0 0 1 .]\n",
    "[1 1 0 x]\n",
    "</pre>\n",
    "\n",
    "Let's create a class for the maze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== ORIGINAL ==\n",
      "[[0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [1 1 0 0]]\n",
      "== RANDOM ====\n",
      "[[0 1 1 0]\n",
      " [0 0 1 0]\n",
      " [1 0 1 1]\n",
      " [0 0 1 0]]\n",
      "note: the random maze may have no solution\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Maze():\n",
    "    \n",
    "    def __init__(self, random=False):\n",
    "        if random:\n",
    "            self.maze = np.random.randint(0,2,size=(4,4))\n",
    "            self.maze[0][0] = 0  # no obstacles allowed at starting point...\n",
    "            self.maze[3][3] = 0  # ...or at ending point.\n",
    "        else:\n",
    "            self.maze = np.array([[0,0,0,1],[0,1,0,0],[0,0,1,0],[1,1,0,0]])  # the initial maze is hardcoded\n",
    "          \n",
    "m = Maze()\n",
    "print('== ORIGINAL ==')\n",
    "print(m.maze)  \n",
    "m = Maze(random=True)\n",
    "print('== RANDOM ====')\n",
    "print(m.maze)\n",
    "print('note: the random maze may have no solution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up the maze to act as an environment for machine learning..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is what happens if you go out of bounds:\n",
      "[ 0 -1] -1 True\n",
      "...and here is the only safe path through the maze:\n",
      "(array([0, 1]), 0, False)\n",
      "(array([0, 2]), 0, False)\n",
      "(array([1, 2]), 0, False)\n",
      "(array([1, 3]), 0, False)\n",
      "(array([2, 3]), 0, False)\n",
      "(array([3, 3]), 1, True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Maze():\n",
    "    \n",
    "    # offsets to move North, South, East, or West\n",
    "    N,S,E,W = (-1,0),(1,0),(0,1),(0,-1)\n",
    "    \n",
    "    def __init__(self, random=False):   \n",
    "        self.reset(random)\n",
    "    \n",
    "    # moved code from __init__ to here, to allow re-use of Maze instance\n",
    "    def reset(self, random=False):\n",
    "        if random:\n",
    "            self.maze = np.random.randint(0,2,size=(4,4))\n",
    "            self.maze[0][0] = 0  # no obstacles allowed at starting point...\n",
    "            self.maze[3][3] = 0  # ...or at ending point.\n",
    "        else:\n",
    "            self.maze = np.array([[0,0,0,1],[0,1,0,0],[0,0,1,0],[1,1,0,0]])  # the initial maze is hardcoded\n",
    "        self.player = np.array([0,0])\n",
    "        return self.player\n",
    "    \n",
    "    # action should be one of: Maze.N, Maze.S, Maze.E, Maze.W\n",
    "    # returns reward, done\n",
    "    # rewards are: +1 = success, -1 = failure, 0 = no outcome\n",
    "    # done = True if a terminal state is reached, otherwise False\n",
    "    def step(self, action):\n",
    "        self.player = np.add(self.player, action)\n",
    "        if max(self.player) > 3 or min(self.player) < 0:      # out of bounds\n",
    "            return self.player, -1, True\n",
    "        elif self.maze[self.player[0]][self.player[1]] != 0:  # moved onto a blocked space\n",
    "            return self.player, -1, True\n",
    "        elif np.array_equal(self.player, (3,3)):              # reached the exit\n",
    "            return self.player, 1, True\n",
    "        else:\n",
    "            return self.player, 0, False                      # no outcome (player is on an open space)\n",
    "    \n",
    "    # return a random action (equally distributed across the action space)\n",
    "    def sample(self):\n",
    "        n = np.random.randint(4)\n",
    "        if n == 0:\n",
    "            return Maze.N\n",
    "        elif n == 1:\n",
    "            return Maze.S\n",
    "        elif n == 2:\n",
    "            return Maze.E\n",
    "        elif n == 3:\n",
    "            return Maze.W\n",
    "\n",
    "print('Here is what happens if you go out of bounds:')\n",
    "e = Maze()\n",
    "observation, reward, done = e.step(Maze.W)\n",
    "print(observation, reward, done)\n",
    "\n",
    "print('...and here is the only safe path through the maze:')\n",
    "e = Maze()\n",
    "print(e.step(Maze.E))\n",
    "print(e.step(Maze.E))\n",
    "print(e.step(Maze.S))\n",
    "print(e.step(Maze.E))\n",
    "print(e.step(Maze.S))\n",
    "print(e.step(Maze.S))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QPlayer():\n",
    "    \n",
    "    EXPLORE = 0.05\n",
    "    ALPHA = 0.10\n",
    "    GAMMA = 0.90\n",
    "    \n",
    "    def __init__(self, explore=EXPLORE, alpha=ALPHA, gamma=GAMMA):\n",
    "        super().__init__()\n",
    "        self.q_table = np.zeros([4*4,4])\n",
    "        self.explore = explore\n",
    "        self.a = alpha\n",
    "        self.g = gamma\n",
    "        \n",
    "    def run(self, environment):\n",
    "        observation = environment.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            state = observation[0] * 4 + observation[1]\n",
    "            action = np.argmax(self.q_table[state])\n",
    "            observation, reward, done = environment.step(action)\n",
    "            future_state = observation[0] * 4 + observation[1]\n",
    "            print(self.q_table)\n",
    "            print(future_state)\n",
    "            self.q_table[state][action] += ((1-self.a) * self.q_table[state] \n",
    "                                            + self.a * (reward + np.amax(self.q_table[future_state])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-18298586a587>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-7c2b79009ba5>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, environment)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             self.q_table[state][action] += ((1-self.a) * self.q_table[state] \n\u001b[0;32m---> 25\u001b[0;31m                                             + self.a * (reward + np.amax(self.q_table[future_state])))\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "q = QPlayer()\n",
    "e = Maze()\n",
    "for n in range(100):\n",
    "    q.run(e)\n",
    "    print(np.sum(q.q_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
